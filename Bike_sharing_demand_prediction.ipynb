{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yraj1996/Bike-Sharing-Demand-Prediction/blob/main/Bike_sharing_demand_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vncDsAP0Gaoa"
      },
      "source": [
        "# **Project Name**    - Bike Sharing Demand Prediction \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beRrZCGUAJYm"
      },
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Individual\n",
        "##### ** Individual name - Yash raj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJNUwmbgGyua"
      },
      "source": [
        "# **Project Summary -**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6v_1wHtG2nS"
      },
      "source": [
        "This Project aims to predict the demand for bike sharing services in Seoul, South Korea. Bike sharing services have become increasingly popular in recent years, providing an affordable and environmental friendly way to get around the city. However, bike sharing companies often struggle with balancing their inventory of bikes at each station, as demand for bikes can fluctuate greatly throughout the day.\n",
        "\n",
        "To address this issue, the Seoul Metropolitan Government has provided data on the city's bike sharing system, including information on weather, time of day, and the number of bikes rented. This data has been used to build a machine learning model that predicts the number of bikes that will be rented at any given time.\n",
        "\n",
        "The project was divided into several stages.\n",
        "\n",
        "* The first stage involved data inspection which included looking for missing values and duplicate values. \n",
        "\n",
        "* The next stage involved exploratory data analysis, where trends and patterns in the data were identified. Distribution of data was also studied. \n",
        "\n",
        "* Hypothesis Testing was performed on two hypothesis. \n",
        "* After this Feature selection and Preprocessing of data was done by converting categorical variables into numerical ones, splitting data and data scaling .\n",
        "\n",
        "* Furthur, various machine learning algorithms were tested on the split data that is trainig and test dataset, including Linear regression, Lasso regression, Ridge regression and Random forests classifier. \n",
        "* The performance of each model was evaluated using metrics such as root mean squared error and mean absolute error and R squared. The Random forests classifier model was found to be the most accurate.\n",
        "\n",
        "Overall, the Seoul Bike Sharing Demand Prediction Project demonstrates the power of machine learning in predicting demand for bike sharing services. By accurately predicting demand, bike sharing companies can optimize their inventory and provide better service to their customers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6K7xa23Elo4"
      },
      "source": [
        "# **GitHub Link -**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1o69JH3Eqqn"
      },
      "source": [
        "Provide your GitHub Link here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQaldy8SH6Dl"
      },
      "source": [
        "# **Problem Statement**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpeJGUA3kjGy"
      },
      "source": [
        "**Currently Rental bikes are introduced in many urban cities for the enhancement of mobility comfort. It is important to make the rental bike available and accessible to the public at the right time as it lessens the waiting time. Eventually, providing the city with a stable supply of rental bikes becomes a major concern. The crucial part is the prediction of bike count required at each hour for the stable supply of rental bikes.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDgbUHAGgjLW"
      },
      "source": [
        "# **General Guidelines** : -  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      },
      "source": [
        "1.   Well-structured, formatted, and commented code is required. \n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits. \n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule. \n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_i_v8NEhb9l"
      },
      "source": [
        "# ***Let's Begin !***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhfV-JJviCcP"
      },
      "source": [
        "## ***1. Know Your Data***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3lxredqlCYt"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "outputs": [],
      "source": [
        "# importing libraries for analysis\n",
        "import numpy as np\n",
        "from numpy import math\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "#importing libraries for hypothesis testing\n",
        "from scipy import stats\n",
        "from scipy.stats import ttest_ind\n",
        "from scipy.stats import f_oneway\n",
        "from statsmodels.stats.weightstats import ztest\n",
        "\n",
        "#importing libraries for preprocessing and ML Implementation.\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RnN4peoiCZX"
      },
      "source": [
        "### Dataset Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "outputs": [],
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "bike_df=pd.read_csv('/content/drive/MyDrive/ALMA BETTER Capstone Project/Bike Sharing Demand/SeoulBikeData.csv',encoding ='unicode_escape')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMs8CloUJCum"
      },
      "source": [
        "We named our dataset as bike_df. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x71ZqKXriCWQ"
      },
      "source": [
        "### Dataset First View"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "outputs": [],
      "source": [
        "# Dataset First Look\n",
        "bike_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdi0Q8pnKidK"
      },
      "outputs": [],
      "source": [
        "#last 5 rows of dataset\n",
        "bike_df.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gFe4ZC_JMfT"
      },
      "source": [
        "Here, we are looking at first view of dataset by using 'dataframe.head' and 'dataframe.tail' function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hBIi_osiCS2"
      },
      "source": [
        "### Dataset Rows & Columns count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "outputs": [],
      "source": [
        "# Dataset Rows & Columns count\n",
        "bike_df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9u6pL2m8IgZt"
      },
      "source": [
        "There are 8760 rows and 14 columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlHwYmJAmNHm"
      },
      "source": [
        "### Dataset Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "outputs": [],
      "source": [
        "# Dataset Info\n",
        "bike_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoUhjYI1I97f"
      },
      "source": [
        "From bike_df.info we can understand about features and  their data types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35m5QtbWiB9F"
      },
      "source": [
        "#### Duplicate Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "outputs": [],
      "source": [
        "# Dataset Duplicate Value Count\n",
        "bike_df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQ3qK_gXLAmK"
      },
      "source": [
        "There is no duplicate value in dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoPl-ycgm1ru"
      },
      "source": [
        "#### Missing Values/Null Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "outputs": [],
      "source": [
        "#null values in dataset\n",
        "bike_df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0kj-8xxnORC"
      },
      "source": [
        "### What did you know about your dataset?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfoNAAC-nUe_"
      },
      "source": [
        "Our dataset contains 8760 rows and 14 columns.\n",
        "\n",
        "No null value is present in the datset.\n",
        "\n",
        "No duplicate observations were present in the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      },
      "source": [
        "## ***2. Understanding Your Variables***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "outputs": [],
      "source": [
        "# Dataset Columns\n",
        "# going to look at columns we have in dataset\n",
        "bike_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "outputs": [],
      "source": [
        "# Dataset Describe\n",
        "bike_df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tY9WQX0MOVFB"
      },
      "source": [
        "dataframe.describe gives some statistical information of numerical features in dataset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBTbrJXOngz2"
      },
      "source": [
        "### Variables Description "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJV4KIxSnxay"
      },
      "source": [
        "Date : year-month-day\n",
        "\n",
        "Rented Bike count - Count of bikes rented at each hour\n",
        "\n",
        "Hour - Hour of the day\n",
        "\n",
        "Temperature- Temperature in Celsius\n",
        "\n",
        "Humidity - Humidity in %\n",
        "\n",
        "Windspeed - Windspeed in m/s\n",
        "\n",
        "Visibility - Visible distance in multiple of 10m\n",
        "\n",
        "Dew point temperature - Celsius\n",
        "\n",
        "Solar radiation - Radiation of sun in MJ/m2\n",
        "\n",
        "Rainfall - Rainfall in mm\n",
        "\n",
        "Snowfall - Snowfall in cm\n",
        "\n",
        "Seasons - Winter, Spring, Summer, Autumn\n",
        "\n",
        "Holiday - Holiday/No holiday\n",
        "\n",
        "Functional Day - NoFunc(Non Functional Hours), Fun(Functional hours)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3PMJOP6ngxN"
      },
      "source": [
        "### Check Unique Values for each variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "outputs": [],
      "source": [
        "# Check Unique Values for each variable.\n",
        "bike_df.nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dauF4eBmngu3"
      },
      "source": [
        "## 3. ***Data Wrangling***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKJF3rekwFvQ"
      },
      "source": [
        "### Data Wrangling Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "outputs": [],
      "source": [
        "# Changing the columns name fo further analysis.\n",
        "bike_df=bike_df.rename(columns={'Rented Bike Count':'Rented_bike_count',\n",
        "                                'Temperature(°C)':'Temperature',\n",
        "                                'Humidity(%)':'Humidity',\n",
        "                                'Wind speed (m/s)':'wind_speed',\n",
        "                                'Visibility (10m)':'Visibility',\n",
        "                                'Dew point temperature(°C)':'Dew_point_temperature',\n",
        "                                'Solar Radiation (MJ/m2)':'Solar_Radiation', \n",
        "                                'Rainfall(mm)':'Rainfall',\n",
        "                                'Snowfall (cm)':'Snowfall',\n",
        "                                'Functioning Day':'Functioning_Day'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1aQNZJTSUAk"
      },
      "outputs": [],
      "source": [
        "#converting date column to datetime format\n",
        "bike_df['Date'] = pd.to_datetime(bike_df['Date'], format=\"%d/%m/%Y\", infer_datetime_format='%d%m%Y', exact=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmhEmza5SnNM"
      },
      "outputs": [],
      "source": [
        "# split day of week, month and year in three column\n",
        "bike_df['day_of_week'] = bike_df['Date'].dt.day_name()   # extract week name from Date column\n",
        "bike_df[\"month\"] = bike_df['Date'].dt.month_name()       # extract month name from Date column\n",
        "bike_df[\"year\"] = bike_df['Date'].map(lambda x: x.year).astype(\"object\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlEBrmgkTEG4"
      },
      "outputs": [],
      "source": [
        "#new column that indicates whether each row corresponds to a weekday or a weekend:\n",
        "bike_df['weekday'] = bike_df['Date'].dt.weekday < 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOxVrbENTmUY"
      },
      "outputs": [],
      "source": [
        "#convert string attributes Funtioning_Day to numerical values(binary value).\n",
        "bike_df['Functioning_Day'] = bike_df['Functioning_Day'].apply(lambda x:1 if x=='Yes' else 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIO8ej4zVSi9"
      },
      "outputs": [],
      "source": [
        "#convert string attributes Holiday to numerical values(Binary).\n",
        "bike_df['Holiday'] = bike_df['Holiday'].apply(lambda x:1 if x=='Holiday' else 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKDqEysQjZuq"
      },
      "outputs": [],
      "source": [
        "bike_df['Holiday'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8MC2ak0jzSc"
      },
      "outputs": [],
      "source": [
        "bike_df['Functioning_Day'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSa1f5Uengrz"
      },
      "source": [
        "### What all manipulations have you done and insights you found?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbyXE7I1olp8"
      },
      "source": [
        "Converted \"Functionioning Day\" column values to binary values.\n",
        "\n",
        "Converted \"Holiday\" column values to binary values.\n",
        "\n",
        "Converted \"Date\" column to Datetime format.\n",
        "\n",
        "Extracted Day of week, Month, Year from date to make separate features.\n",
        "\n",
        "Introduced a column \"weekday\" that represents whether there was a weekend or weekday on respective date."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF8Ens_Soomf"
      },
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "626xLvZ0ZOga"
      },
      "source": [
        "### Univariate Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wOQAZs5pc--"
      },
      "source": [
        "#### Chart - 1- Histogram on target variable "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "outputs": [],
      "source": [
        "#Disribution of target variable\n",
        "plt.figure(figsize=(15,8))\n",
        "sns.distplot(bike_df['Rented_bike_count'],color='forestgreen')\n",
        "plt.xlabel('Rented_bike_count')\n",
        "plt.title('Rented_bike_count distribution')\n",
        "plt.axvline(bike_df['Rented_bike_count'].mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
        "plt.axvline(bike_df['Rented_bike_count'].median(), color='cyan', linestyle='dashed', linewidth=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5QZ13OEpz2H"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XESiWehPqBRc"
      },
      "source": [
        "A Distplot or distribution plot, depicts the variation in the data distribution.It is used basically for univariant set of observations and visualizes that is through a histogram. So, i used distplot to see distribution of dependent variable. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_j1G7yiqdRP"
      },
      "source": [
        "* Dependent variable distribution is slightly right skewed.\n",
        "* Maximum frequency of rented bike count were of values less than 500\n",
        "* Median lies around 500 and 75 percentile lies around 1000."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "448CDAPjqfQr"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cspy4FjqxJW"
      },
      "source": [
        "Just a histogram cannot define business impact. It's done just to see the distribution of the column data over the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSlN3yHqYklG"
      },
      "source": [
        "#### Chart - 2 - Distribution of Independent Variables "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "outputs": [],
      "source": [
        "# Chart - 2 visualization code\n",
        "#plotting distplot for various features\n",
        "numerical_columns=list(['Temperature', 'Humidity', 'wind_speed',\n",
        "       'Visibility', 'Dew_point_temperature', 'Solar_Radiation', 'Rainfall',\n",
        "       'Snowfall'])\n",
        "\n",
        "for col in numerical_columns:\n",
        "  plt.figure(figsize=(8,10))\n",
        "  sns.distplot(x=bike_df[col],color='y')\n",
        "  plt.axvline(bike_df[col].mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
        "  plt.axvline(bike_df[col].median(), color='cyan', linestyle='dashed', linewidth=2)\n",
        "  plt.xlabel(col)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6dVpIINYklI"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aaW0BYyYklI"
      },
      "source": [
        "The histogram is a popular graphing tool. It is used to summarize discrete or continuous data that are measured on an interval scale. It is often used to illustrate the major features of the distribution of the data in a convenient form. It is also useful when dealing with large data sets (greater than 100 observations). It can help detect any unusual observations (outliers) or any gaps in the data.Thus, I used the histogram plot to analysis the variable distributions over the whole dataset whether it's symmetric or not.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijmpgYnKYklI"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSx9atu2YklI"
      },
      "source": [
        "Just a histogram and box plot cannot define business impact. It's done just to see the distribution of the column data over the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JiQyfWJYklI"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcBbebzrYklV"
      },
      "source": [
        "Answer Here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DJ5hjbDiu4g"
      },
      "outputs": [],
      "source": [
        "bike_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9py2cVki_er"
      },
      "outputs": [],
      "source": [
        "bike_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM7whBJCYoAo"
      },
      "source": [
        "#### Chart - 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "outputs": [],
      "source": [
        "# Chart - 3 visualization code\n",
        "#plotting barchart for categorical columns\n",
        "categorical=['Seasons', 'Holiday', 'Functioning_Day']\n",
        "for i in categorical:\n",
        "  sns.countplot(data=bike_df, x=i)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fge-S5ZAYoAp"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dBItgRVYoAp"
      },
      "source": [
        "A bar chart or bar graph is a chart or graph that presents categorical data with rectangular bars with heights or lengths proportional to the values that they represent. A bar chart is used when you want to show a distribution of data points or perform a comparison of metric values across different subgroups of your data. From a bar chart, we can see which groups are highest or most common, and how other groups compare against the others.\n",
        "\n",
        "Thus, I used bar chart to show and compare the count among different seasons, Holiday and Fuctioning day."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85gYPyotYoAp"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jstXR6OYoAp"
      },
      "source": [
        "* Observations for all four seasons are fairly equal.\n",
        "* Observations for Holidays are very less compared to Non- Holiday observations.\n",
        "* Observations for Functioning days are very large and Non functioning day are very less."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoGjAbkUYoAp"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      },
      "source": [
        "It's done just to see the distribution of the column data over the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kbp6J7wkkpLU"
      },
      "source": [
        "### Bivariate Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Of9eVA-YrdM"
      },
      "source": [
        "#### Chart - 4- Understanding relationship of variables(numerical) with target variable(Bivariate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hBEtyP4k5UQ"
      },
      "outputs": [],
      "source": [
        "bike_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "outputs": [],
      "source": [
        "#Understanding relationship of variables with target variable using regplot\n",
        "for i in numerical_columns:\n",
        "  sns.regplot(bike_df, x=i, y='Rented_bike_count',line_kws={\"color\": \"black\"},scatter_kws={\"color\": 'springgreen'})\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iky9q4vBYrdO"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJRCwT6DYrdO"
      },
      "source": [
        "Scatter plots are the graphs that present the relationship between two variables in a data-set. It represents data points on a two-dimensional plane or on a Cartesian system. The independent variable or attribute is plotted on the X-axis, while the dependent variable is plotted on the Y-axis.\n",
        "\n",
        "Regplot is a function of scatter plot, which helps us to understand the relationship between independent variable with respect to target variable.\n",
        "seaborn.regplot() method is used to plot data and draw a linear regression model fit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6T5p64dYrdO"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      },
      "source": [
        "\n",
        "* General trend is as the temperature increases number of bike rentals also increases.\n",
        "* Bike rental count decreases with increase in Humidity.\n",
        "* Bike Rentals are fairly invariable of Wind speed but a faint increase can be seen in the trendline.\n",
        "* Bike Rentals mildly increases with increase in Visibility\n",
        "* Sharp increase can be noted with increase in Dew Point temperature.\n",
        "* On an average, Rented bike counts increases with increase in Solar Radiation.\n",
        "* Decline in Bike Rentals is seen with increase in Rainfall.\n",
        "* Decline in Bike Rentals is seen with increase in Snowfall."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-Ehk30pYrdP"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLNxxz7MYrdP"
      },
      "source": [
        "For Bike sharing services these data's and trends are very cruicial in understanding the \"How natural environment agents create impact on Bike renting\"\n",
        "From above scatter plot charts you can understand how customer perefer renting a bike when tempearture is mild and pleasent, less rainfall and snowfall, when visibility is good and wind speed should be at ideal condition.\n",
        "Bike sahring companies can manage supplies and outlet points according to this trends."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bamQiAODYuh1"
      },
      "source": [
        "#### Chart - 5- Relationship between Target variable and Weekday (Bivariate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "outputs": [],
      "source": [
        "sns.barplot(x='weekday', y='Rented_bike_count', data=bike_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcxuIMRPYuh3"
      },
      "source": [
        "A bar chart or bar graph is a chart or graph that presents categorical data with rectangular bars with heights or lengths proportional to the values that they represent.So, I pick a bar graph to perform a comparison of metric values across different subgroups of Weekday."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwzvFGzlYuh3"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyqkiB8YYuh3"
      },
      "source": [
        "Bike Rentals on weekdays are a little higher than on weekends. This might be due to the usage of Bike Rentals for commuting to office."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYpmQ266Yuh3"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      },
      "source": [
        "This graph tells the Bike renting companies that most of people used bike services on weeday instead on weekends. It indicates that people might using bike rental sevices for commuting to their work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH-pJp9IphqM"
      },
      "source": [
        "#### Chart - 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "outputs": [],
      "source": [
        "sns.barplot(x='Holiday',y='Rented_bike_count',data=bike_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbFf2-_FphqN"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loh7H2nzphqN"
      },
      "source": [
        "A bar chart or bar graph is a chart or graph that presents categorical data with rectangular bars with heights or lengths proportional to the values that they represent.So, I pick a bar graph to perform a comparison of metric values across different subgroups of Holiday."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ouA3fa0phqN"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VECbqPI7phqN"
      },
      "source": [
        " Rented Bikes count is way more on working days than on Holidays. This might due to the usage of Rental Bikes for commuting to office."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Seke61FWphqN"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW4_bGpfphqN"
      },
      "source": [
        "This charts shows that people rent bikes on working days for commuting to their work. This may help Bike sharing companies to understand the usage and can create sales plans and  it also helps them to manage their bike inventory on each station according to trend."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIIx-8_IphqN"
      },
      "source": [
        "#### Chart - 7 -Bike rent count in different seasons(Bivariate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "outputs": [],
      "source": [
        "sns.barplot(data=bike_df,x='Seasons',y='Rented_bike_count')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t27r6nlMphqO"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iv6ro40sphqO"
      },
      "source": [
        "A bar chart or bar graph is a chart or graph that presents categorical data with rectangular bars with heights or lengths proportional to the values that they represent.So, I pick a bar graph to perform a comparison of metric values across different subgroups of seasons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2jJGEOYphqO"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po6ZPi4hphqO"
      },
      "source": [
        "* Highest Mean Rentals were observed during Summer.\n",
        "* Lowest Rentals were observed in Winters. This might be due to high snowfall and subzero temperature in city and also the holiday season during month of december."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0JNsNcRphqO"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvSq8iUTphqO"
      },
      "source": [
        "This charts shows that people rent bikes more in comaprison to other seasons. Whereas bike service facing heavy decline in winter. This may help Bike sharing companies to understand the usage and can create sales plans and  it also helps them to manage their bike inventory on each station according to seasons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZR9WyysphqO"
      },
      "source": [
        "#### Chart - 8- Bike rent count on different days of week.(Bivariate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "outputs": [],
      "source": [
        "sns.barplot(x='day_of_week',data=bike_df, y='Rented_bike_count')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj7wYXLtphqO"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ob8u6rCTphqO"
      },
      "source": [
        "A bar chart or bar graph is a chart or graph that presents categorical data with rectangular bars with heights or lengths proportional to the values that they represent.So, I pick a bar graph to perform a comparison of metric values across different subgroups of day_of_week."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZrbJ2SmphqO"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZtgC_hjphqO"
      },
      "source": [
        "Bike Rentals were fairly equal on all days with a decline in Rentals on Sundays."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFu4xreNphqO"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ey_0qi68phqO"
      },
      "source": [
        "All these Bivariate analysis helps bike renting companies to understand the bike usage according to seasons. In which seasons they need to preapre more and needs to deploy startegies to manage their inventories and plans."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJ55k-q6phqO"
      },
      "source": [
        "#### Chart - 9- Bike rent count in different months(Bivariate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6CZ_H-LDTUf"
      },
      "outputs": [],
      "source": [
        "plt.subplots(figsize=(15,8))\n",
        "sns.barplot(x='month',data=bike_df, y='Rented_bike_count')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCFgpxoyphqP"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVxDimi2phqP"
      },
      "source": [
        "A bar chart helps to plot and evaluate the metric of subgroups of a certain features. So I choose bar graph to plot count of bike rent with respects to given months."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVtJsKN_phqQ"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngGi97qjphqQ"
      },
      "source": [
        "* Least Rentals were observed in the month of December, January and February \n",
        "i.e. during winter seasons.\n",
        "* Highest Rentals were observed during the months of May, June, July i.e. during summer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lssrdh5qphqQ"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBpY5ekJphqQ"
      },
      "source": [
        "This analysis leads to create a positive impact as company knows which months are going to be more busy and they can plan their strategy according to that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      },
      "source": [
        "#### Chart - 10- Bike rent count at different Hours(Bivariate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(18,6))\n",
        "plt.grid()\n",
        "sns.scatterplot(bike_df, x='Hour', y='Rented_bike_count', alpha=0.2)\n",
        "plt.ylabel(\"Rented Bike count\")\n",
        "plt.title('Hour vs Rented Bike count')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHt1CE9nFaU4"
      },
      "outputs": [],
      "source": [
        "plt.subplots(figsize=(15,8))\n",
        "sns.lineplot(x='Hour',data=bike_df, y='Rented_bike_count',linestyle='-',marker='o')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M8mcRywphqQ"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8agQvks0phqQ"
      },
      "source": [
        "Scatter plots are the graphs that present the relationship between two variables in a data-set. It represents data points on a two-dimensional plane or on a Cartesian system. The independent variable or attribute is plotted on the X-axis, while the dependent variable is plotted on the Y-axis. So, I choose scatter plot to show the relationship between target variable and Hour.\n",
        "\n",
        "I also use Line chart to track changes over time. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgIPom80phqQ"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qp13pnNzphqQ"
      },
      "source": [
        "* Demand of Bikes is minimum from Midnight to 6 A.M.\n",
        "* Sharp increase in demand can be seen from 7 A.M. to 8 A.M. then declines. This might be due to office and school goers.\n",
        "* Steady increase in demand is witnessed from 11 AM till 4 PM.\n",
        "* Sudden spike in rentals is seen from 4 PM to 6 PM possibly indicating return of office goers.\n",
        "Rental demand thereafter decreases steadily."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMzcOPDDphqR"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4Ka1PC2phqR"
      },
      "source": [
        "Yes, the insights found will definitely help for a positive business impact. \n",
        "From these insights bike renting sevice can manage their inventory in better manner.\n",
        "Most of demand can be seen at peak morning office hours and evening hours when people return their home from work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq0VychFGVb7"
      },
      "source": [
        "### Multivariate Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-EpHcCOp1ci"
      },
      "source": [
        "#### Chart 11 - Relationship between target variable, Hour and day of week."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(18,6))\n",
        "plt.grid()\n",
        "sns.pointplot(bike_df, x='Hour', y='Rented_bike_count', markers='o', hue='day_of_week')\n",
        "plt.ylabel(\"Rented Bike count\")\n",
        "plt.title('Hour vs Rented Bike count vs days')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_VqEhTip1ck"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vsMzt_np1ck"
      },
      "source": [
        "I pick the line chart which depict the change in rented bike count over time on respective days. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zGJKyg5p1ck"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      },
      "source": [
        "* Sudden spike is seen between 7-9 AM and 5-7 PM. This is due to the office goers and students.\n",
        "* Rentals after midnight are more on weekends than weekdays.\n",
        "* Rentals during day is more on weekdays than weekends and sudden jump is also not witnessed during weekends."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "druuKYZpp1ck"
      },
      "source": [
        "yes this insight definately help to create a positive impact on business as companies know who are their prime customers and when they need bikes most.\n",
        "They can also manage their inventories and station according to the above insights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3dbpmDWp1ck"
      },
      "source": [
        "#### Chart - 12- Relationship between target variable, Hour and seasons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,6))\n",
        "plt.grid()\n",
        "sns.pointplot(bike_df, x='Hour', y='Rented_bike_count', markers='o', hue='Seasons')\n",
        "plt.ylabel(\"Rented Bike count\")\n",
        "plt.title('Hour vs Rented Bike count vs seasons')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylSl6qgtp1ck"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2xqNkiQp1ck"
      },
      "source": [
        "Line chart easily shows the changes over time.So, I pick line chart to show change in bike rent count of different season at different hours."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWILFDl5p1ck"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-lUsV2mp1ck"
      },
      "source": [
        "* Peak hours remain to be the same irrespective of the seasons.\n",
        "* Winters have least numbers of rentals followed by Spring, Autumn and Summer respectively.\n",
        "* Demand after 6 PM is marginally high during winters than other seasons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7G43BXep1ck"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wwDJXsLp1cl"
      },
      "source": [
        "This insight is very helpful for bike rental companies as they knwo whhich seasons and hours are more important and needs to target.\n",
        "they can plan some offer and manage their inventories according to above insights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NC_X3p0fY2L0"
      },
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "outputs": [],
      "source": [
        "# Correlation Heatmap visualization code\n",
        "plt.figure(figsize=(12,10))\n",
        "sns.heatmap(bike_df.corr().abs(), annot=True,cmap='coolwarm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      },
      "source": [
        "A correlation matrix is a table showing correlation coefficients between variables. Each cell in the table shows the correlation between two variables. A correlation matrix is used to summarize data, as an input into a more advanced analysis, and as a diagnostic for advanced analyses. The range of correlation is [-1,1].\n",
        "\n",
        "Thus to know the correlation between all the variables along with the correlation coeficients, i used correlation heatmap."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfSqtnDqZNRR"
      },
      "source": [
        "From the above correlation heatmap, we can say that - Dew Point Temperature and Temperature are highly correalted thus one of them should be removed before applying linear regression.\n",
        "\n",
        "Rest all correlation can be depicted from the above chart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q29F0dvdveiT"
      },
      "source": [
        "#### Chart - 15 - Pair Plot "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "outputs": [],
      "source": [
        "\n",
        "sns.pairplot(data=bike_df,hue=\"Rented_bike_count\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXh0U9oCveiU"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMmPjTByveiU"
      },
      "source": [
        "Pair plot is used to understand the best set of features to explain a relationship between two variables or to form the most separated clusters.\n",
        "\n",
        "Thus, I used pair plot to analyse the patterns of data and realationship between the features. It's exactly same as the correlation map but here you will get the graphical representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22aHeOlLveiV"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPQ8RGwHveiV"
      },
      "source": [
        "Answer Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-ATYxFrGrvw"
      },
      "source": [
        "## ***5. Hypothesis Testing***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      },
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7MS06SUHkB-"
      },
      "source": [
        "* The average number of bike rentals is different on weekends than on weekdays.\n",
        "* There is a significant difference in the average number of bike rentals during different seasons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yEUt7NnHlrM"
      },
      "source": [
        "### Hypothetical Statement - 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      },
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HI9ZP0laH0D-"
      },
      "source": [
        "* Null Hypothesis: Average bike rentals is same on Weekend as well as Weekdays.\n",
        "\n",
        "* Alternative Hypothesis: Average bike rentals is different on Weekend and Weekdays.\n",
        "* Test type:  z-test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I79__PHVH19G"
      },
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "outputs": [],
      "source": [
        "#split the data into two groups: one for weekdays and one for weekends\n",
        "weekday_rentals = bike_df.loc[bike_df['weekday'], 'Rented_bike_count']\n",
        "weekend_rentals = bike_df.loc[~bike_df['weekday'], 'Rented_bike_count']\n",
        "\n",
        "#perform a two-sample t-test to compare the means of the two groups\n",
        "t,p = ztest(weekday_rentals, weekend_rentals)\n",
        "\n",
        "# Print the results\n",
        "print('t-statistic:', t)\n",
        "print('p-value:', p)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZdUfxQKLxSA"
      },
      "outputs": [],
      "source": [
        "# Determine if the p-value is significant at the 5% level\n",
        "if p < 0.05:\n",
        "    print('Reject the null hypothesis:')\n",
        "    print('The mean number of bike rentals is significantly different on weekends compared to weekdays.')\n",
        "else:\n",
        "    print('Fail to reject the null hypothesis:')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou-I18pAyIpj"
      },
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2U0kk00ygSB"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF3858GYyt-u"
      },
      "source": [
        "##### Why did you choose the specific statistical test?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HO4K0gP5y3B4"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_0_7-oCpUZd"
      },
      "source": [
        "### Hypothetical Statement - 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwyV_J3ipUZe"
      },
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      },
      "source": [
        "There is a significant difference in the average number of bike rentals during different seasons.\n",
        "Null Hypothesis:  Average number of bike rentals is same during different seasons.\n",
        "\n",
        "Alternative Hypothesis:  Average number of bike rentals is different during different seasons.\n",
        "\n",
        "Test type: ANOVA test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yB-zSqbpUZe"
      },
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "outputs": [],
      "source": [
        "# Separate the data into four groups (one for each season)\n",
        "spring_rentals = bike_df.loc[bike_df['Seasons'] == 'Spring', 'Rented_bike_count']\n",
        "summer_rentals = bike_df.loc[bike_df['Seasons'] == 'Summer', 'Rented_bike_count']\n",
        "fall_rentals = bike_df.loc[bike_df['Seasons'] == 'Autumn', 'Rented_bike_count']\n",
        "winter_rentals = bike_df.loc[bike_df['Seasons'] == 'Winter', 'Rented_bike_count']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9H7zoo2MkUd"
      },
      "outputs": [],
      "source": [
        "# Perform an ANOVA test\n",
        "f_statistic, p_value = f_oneway(spring_rentals, summer_rentals, fall_rentals, winter_rentals)\n",
        "\n",
        "print('F-statistic:', f_statistic)\n",
        "print('p-value:', p_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQpq5yz2M6N_"
      },
      "outputs": [],
      "source": [
        "# Determine if the p-value is significant at the 5% level\n",
        "if p_value < 0.05:\n",
        "    print('We reject the null hypothesis:')\n",
        "else:\n",
        "    print('We fail to reject the null hypothesis:')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUvejAfpUZe"
      },
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLDrPz7HpUZf"
      },
      "source": [
        "We use ANOVA test, which use F-statistics F-tests to statistically assess the equality of means when you have three or more groups."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd15vwWVpUZf"
      },
      "source": [
        "##### Why did you choose the specific statistical test?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xOGYyiBpUZf"
      },
      "source": [
        "ANOVA is to test for differences among the means of the population by examining the amount of variation within each sample, relative to the amount of variation between the samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn_IUdTipZyH"
      },
      "source": [
        "### Hypothetical Statement - 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49K5P_iCpZyH"
      },
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gWI5rT9pZyH"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nff-vKELpZyI"
      },
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "outputs": [],
      "source": [
        "# Perform Statistical Test to obtain P-Value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLW572S8pZyI"
      },
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytWJ8v15pZyI"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWbDXHzopZyI"
      },
      "source": [
        "##### Why did you choose the specific statistical test?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M99G98V6pZyI"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLjJCtPM0KBk"
      },
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiyOF9F70UgQ"
      },
      "source": [
        "### 1. Handling Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "outputs": [],
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "bike_df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wuGOrhz0itI"
      },
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ixusLtI0pqI"
      },
      "source": [
        "There are no missing values to handle in the given dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id1riN9m0vUs"
      },
      "source": [
        "### 2. Handling Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "outputs": [],
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "numerical_columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cigsESAcxs29"
      },
      "outputs": [],
      "source": [
        "plt.subplots(figsize=(15,6))\n",
        "sns.boxplot(data=bike_df['Rented_bike_count'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0DpcWwaxyHi"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import norm\n",
        "\n",
        "# Removing \"Date\" variable from numerical variable\n",
        "num_var=[var for var in numerical_columns]\n",
        "\n",
        "# Plotting Box and Distribution plot \n",
        "for var in num_var:\n",
        "    plt.figure(figsize=(15,6))\n",
        "    plt.subplot(1,2,1)\n",
        "    ax=sns.boxplot(data=bike_df[var])\n",
        "    ax.set_title(f'{var}')\n",
        "    ax.set_ylabel(var)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRWOfDLgysB0"
      },
      "outputs": [],
      "source": [
        "outlier_var=['Rented_bike_count', 'wind_speed', 'Solar_Radiation']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyD1bH19yw7w"
      },
      "outputs": [],
      "source": [
        "for i in outlier_var:\n",
        "    # Finding IQR\n",
        "    Q1=bike_df[i].quantile(0.25)\n",
        "    Q3=bike_df[i].quantile(0.75)\n",
        "    IQR=Q3-Q1\n",
        "    \n",
        "    # Defining upper and lower limit\n",
        "    lower_limit =bike_df[i].quantile(0.25)-1.5*IQR\n",
        "    upper_limit =bike_df[i].quantile(0.75)+1.5*IQR\n",
        "    \n",
        "    # Applying lower and upper limit to each variables\n",
        "    bike_df.loc[(bike_df[i] > upper_limit),i] = upper_limit\n",
        "    bike_df.loc[(bike_df[i] < lower_limit),i] = lower_limit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEnuEVzIyzoM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2h7KG5unDDNE"
      },
      "outputs": [],
      "source": [
        "\n",
        "for var in outlier_var:\n",
        "    plt.subplots(figsize=(15,6))\n",
        "    \n",
        "    ax=sns.boxplot(data=bike_df[var])\n",
        "    ax.set_title(f'{var}')\n",
        "    ax.set_ylabel(var)\n",
        "  \n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "578E2V7j08f6"
      },
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DwpFfvu4dkh"
      },
      "source": [
        "By plotting a box plot for target variable and numerical columns we can easily distinguish the features with outliers.\n",
        "After finding the features who has outliers, we use IQR method treat outliers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGZz5OrT1HH-"
      },
      "source": [
        "The box plot is a useful graphical display for describing the behavior of the data in the middle as well as at the ends of the distributions. The box plot uses the median and the lower and upper quartiles (defined as the 25th and 75th percentiles). If the lower quartile is Q1 and the upper quartile is Q3, then the difference (Q3 — Q1) is called the interquartile range or IQR. A box plot is constructed by drawing a box between the upper and lower quartiles with a solid line drawn across the box to locate the median.\n",
        "\n",
        "> We define the Upper and lower boundry for respective datapoints:\n",
        "* lower_limit =quantile(0.25)-1.5*IQR\n",
        "* upper_limit =quantile(0.75)+1.5*IQR\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      },
      "source": [
        "### 4. Feature Manipulation & Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C74aWNz2AliB"
      },
      "source": [
        "#### 1. Feature Manipulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "outputs": [],
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "df=bike_df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXXP9SDE5Kpk"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9p801SOh4069"
      },
      "outputs": [],
      "source": [
        "new_df = df.drop(columns=['Date','Dew_point_temperature','weekday','year'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DejudWSA-a0"
      },
      "source": [
        "#### 2. Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "outputs": [],
      "source": [
        "#Multicollinearity\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "def calc_vif(X):\n",
        "\n",
        "    # Calculating VIF\n",
        "    vif = pd.DataFrame()\n",
        "    vif[\"variables\"] = X.columns\n",
        "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "\n",
        "    return(vif)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6b3GaWF5yNB"
      },
      "outputs": [],
      "source": [
        "calc_vif(new_df[[i for i in new_df.describe().columns if i not in ['Rented_bike_count'] ]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmxU6FpiLRhO"
      },
      "source": [
        "Let's only keep variables with VIF values upto 5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bf2UHH8I6I1U"
      },
      "outputs": [],
      "source": [
        "new_df =new_df.drop(columns=['Functioning_Day'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9PQHErw6TuW"
      },
      "outputs": [],
      "source": [
        "calc_vif(new_df[[i for i in new_df.describe().columns ]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEMng2IbBLp7"
      },
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      },
      "source": [
        "I choose VIF method to find multicollinearity among features. \n",
        "\n",
        "A variance inflation factor (VIF) is a measure of the amount of multicollinearity in regression analysis. Multicollinearity exists when there is a correlation between multiple independent variables in a multiple regression model. This can adversely affect the regression results. Thus, the variance inflation factor can estimate how much the variance of a regression coefficient is inflated due to multicollinearity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      },
      "source": [
        "##### Which all features you found important and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGgaEstsBnaf"
      },
      "source": [
        "I tried to keep VIF upto 5 and for that i drop Functioning_day feature.\n",
        "Apart from feature Functioing day every features are independent and below 5. \n",
        "I found out 10 indepenedent features which are important and validate through VIF and Correlation map( EDA PART)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89xtkJwZ18nB"
      },
      "source": [
        "### 3. Categorical Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXtpORMI2IHL"
      },
      "outputs": [],
      "source": [
        "new_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2E53HYbArw_"
      },
      "outputs": [],
      "source": [
        "cols=['Hour']\n",
        "for col in cols:\n",
        "  df[col]=df[col].astype('category')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpAKaYc3yKzV"
      },
      "outputs": [],
      "source": [
        "new_df['Hour']=new_df['Hour'].astype('category')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "outputs": [],
      "source": [
        "# Encode your categorical columns\n",
        "categorical_features=list(new_df.select_dtypes(['object','category']).columns)\n",
        "categorical_features=pd.Index(categorical_features)\n",
        "categorical_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnwXg_3HzGG6"
      },
      "outputs": [],
      "source": [
        "new_df = pd.get_dummies(new_df, columns=['month','day_of_week','Hour','Seasons'], prefix=[None,None,'hour',None])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Gk-Zx_yzSE2"
      },
      "outputs": [],
      "source": [
        "new_df.columns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67NQN5KX2AMe"
      },
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDaue5h32n_G"
      },
      "source": [
        "I used One hot encoding where the Pandas get dummies function, pd. get_dummies() , allows us to easily one-hot encode our categorical data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iwf50b-R2tYG"
      },
      "source": [
        "### 4. Textual Data Preprocessing \n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMQiZwjn3iu7"
      },
      "source": [
        "#### 1. Expand Contraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "outputs": [],
      "source": [
        "# Expand Contraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVIkgGqN3qsr"
      },
      "source": [
        "#### 2. Lower Casing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "outputs": [],
      "source": [
        "# Lower Casing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkPnILGE3zoT"
      },
      "source": [
        "#### 3. Removing Punctuations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "outputs": [],
      "source": [
        "# Remove Punctuations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hlsf0x5436Go"
      },
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "outputs": [],
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mT9DMSJo4nBL"
      },
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "outputs": [],
      "source": [
        "# Remove Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "outputs": [],
      "source": [
        "# Remove White spaces"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c49ITxTc407N"
      },
      "source": [
        "#### 6. Rephrase Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "outputs": [],
      "source": [
        "# Rephrase Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeJFEK0N496M"
      },
      "source": [
        "#### 7. Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "outputs": [],
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ExmJH0g5HBk"
      },
      "source": [
        "#### 8. Text Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "outputs": [],
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJNqERVU536h"
      },
      "source": [
        "##### Which text normalization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9jKVxE06BC1"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5UmGsbsOxih"
      },
      "source": [
        "#### 9. Part of speech tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "outputs": [],
      "source": [
        "# POS Taging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      },
      "source": [
        "#### 10. Text Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "outputs": [],
      "source": [
        "# Vectorizing Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBMux9mC6MCf"
      },
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "su2EnbCh6UKQ"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNVZ9zx19K6k"
      },
      "source": [
        "### 5. Data Transformation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqoHp30x9hH9"
      },
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujmrXAo8S3xd"
      },
      "source": [
        "Yeah we need to Transform Target variable , Rented Bike Count has moderate right skewness. Since the assumption of linear regression is that 'the distribution of dependent variable has to be normal', so we should perform some operation to make it normal.\n",
        "\n",
        "> Since we have generic rule of applying Square root for the skewed variable in order to make it normal .After applying Square root to the skewed Rented Bike Count, here we get almost normal distribution.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDVYeoNr2RaK"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "sns.distplot(np.sqrt(new_df['Rented_bike_count']),color='forestgreen')\n",
        "plt.xlabel('Rented_bike_count')\n",
        "plt.title('Rented_bike_count distribution')\n",
        "plt.axvline(np.sqrt(new_df['Rented_bike_count'].mean()), color='magenta', linestyle='dashed', linewidth=2)\n",
        "plt.axvline(np.sqrt(new_df['Rented_bike_count'].median()), color='cyan', linestyle='dashed', linewidth=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qiDjFY_2arl"
      },
      "outputs": [],
      "source": [
        "(np.sqrt(new_df['Rented_bike_count'])).skew()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhH2vgX9EjGr"
      },
      "source": [
        "### 8. Data Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "outputs": [],
      "source": [
        "X = new_df.drop(columns=['Rented_bike_count'], axis=1)\n",
        "y = np.sqrt(new_df['Rented_bike_count'])\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=0)\n",
        "print(X_test.shape)\n",
        "print(X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjKvONjwE8ra"
      },
      "source": [
        "##### What data splitting ratio have you used and why? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      },
      "source": [
        "Before, fitting any model it is a rule of thumb to split the dataset into a training and test set. This means some proportions of the data will go into training the model and some portion will be used to evaluate how our model is performing on any unseen data. The proportions may vary from 60:40, 70:30, 75:25 depending on the person but I used is 75:25 for training and testing respectively. \n",
        "\n",
        "In this step we  split our data into training and testing set using scikit learn library "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMDnDkt2B6du"
      },
      "source": [
        "### 6. Data Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "outputs": [],
      "source": [
        "# Scaling your data\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFlVqW699NgI"
      },
      "outputs": [],
      "source": [
        "X_train[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftthKcDe9Qzk"
      },
      "outputs": [],
      "source": [
        "X_test[0:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiiVWRdJDDil"
      },
      "source": [
        "##### Which method have you used to scale you data and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOQ3d0R28vxE"
      },
      "source": [
        "Scaling helps to standardize the range of features and ensure that each feature (continuous variable) contributes equally to the analysis.\n",
        "I used Min-Max scaler which scales the data to a fixed range, typically between 0 and 1. It helps us in to normalise the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UUpS68QDMuG"
      },
      "source": [
        "### 7. Dimesionality Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kexQrXU-DjzY"
      },
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "outputs": [],
      "source": [
        "# DImensionality Reduction (If needed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5CmagL3EC8N"
      },
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKr75IDuEM7t"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1XJ9OREExlT"
      },
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFOzZv6IFROw"
      },
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeKDIv7pFgcC"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "outputs": [],
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIqpNgepFxVj"
      },
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbet1HwdGDTz"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfCC591jGiD4"
      },
      "source": [
        "## ***7. ML Model Implementation***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      },
      "source": [
        "### 1-Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u75wZOb8Fyqa"
      },
      "source": [
        "Linear regression is one of the most basic types of regression in\n",
        "supervised machine learning. The linear regression model consists of a\n",
        "predictor variable and a dependent variable related linearly to each\n",
        "other. We try to find the relationship between independent variable\n",
        "(input) and a corresponding dependent variable (output)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "outputs": [],
      "source": [
        "# Fitting Multiple Linear Regression to the Training set\n",
        "from sklearn.linear_model import LinearRegression\n",
        "reg= LinearRegression().fit(X_train, y_train)\n",
        "\n",
        "reg.score(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aphldwt0dx-a"
      },
      "outputs": [],
      "source": [
        "reg.coef_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRPCOB7vf_cv"
      },
      "outputs": [],
      "source": [
        "y_pred_train=reg.predict(X_train)\n",
        "y_pred_test=reg.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlgjZdUtgB7E"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "LR_train_mse=mean_squared_error(y_train,y_pred_train)\n",
        "print(\"MSE:\",LR_train_mse)\n",
        "\n",
        "LR_train_rmse=np.sqrt(LR_train_mse)\n",
        "print(\"RMSE:\",LR_train_rmse)\n",
        "\n",
        "LR_train_mae=mean_absolute_error(y_train,y_pred_train)\n",
        "print(\"MAE:\",LR_train_mae)\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "LR_train_R2=r2_score(y_train,y_pred_train)\n",
        "print(\"R2:\",LR_train_R2)\n",
        "Adjusted_LR_train_R2 = (1-(1-r2_score(y_train, y_pred_train))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print('adjusted R2:',Adjusted_LR_train_R2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sOig8dbgvAq"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "LR_test_mse=mean_squared_error(y_test,y_pred_test)\n",
        "print(\"MSE:\",LR_test_mse)\n",
        "\n",
        "LR_test_rmse=np.sqrt(LR_test_mse)\n",
        "print(\"RMSE:\",LR_test_rmse)\n",
        "\n",
        "LR_test_mae=mean_absolute_error(y_test,y_pred_test)\n",
        "print(\"MAE:\",LR_test_mae)\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "LR_test_R2=r2_score(y_test,y_pred_test)\n",
        "print(\"R2:\",LR_test_R2)\n",
        "Adjusted_LR_test_R2 = (1-(1-r2_score(y_test, y_pred_test))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print('adjusted R2:',Adjusted_LR_test_R2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igKhp8Vog4GG"
      },
      "outputs": [],
      "source": [
        "#checking for heteroscedasticity\n",
        "residuals_train = y_train - y_pred_train\n",
        "residuals_test = y_test - y_pred_test\n",
        "\n",
        "plt.subplots(figsize=(10,5))\n",
        "plt.scatter(y_pred_train, residuals_train,c='yellow')\n",
        "plt.plot(y, [0]*len(y),color='green')\n",
        "plt.title('Scatter plot between residual and actual count')\n",
        "plt.xlabel('Predicted count')\n",
        "plt.ylabel('Residual')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ynZpElKhDg9"
      },
      "outputs": [],
      "source": [
        "#mean of residuals\n",
        "round((np.mean(residuals_train)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArJBuiUVfxKd"
      },
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "outputs": [],
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.plot(y_pred_test[0:100])\n",
        "plt.plot(np.array(y_test[0:100]))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1_NZeG_-UB9"
      },
      "source": [
        "I used simple Linear regression algorithm as first machine learning model.\n",
        "\n",
        "For training dataset, I found out the following:\n",
        "* MSE: 56.060321835040796\n",
        "* RMSE: 7.487344110900794\n",
        "* MAE: 5.255209533889809\n",
        "* R2: 0.6284600194562888\n",
        "* adjusted R2: 0.6188842467618634\n",
        "\n",
        "Where as for test dataset, I found out the following:\n",
        "* MSE: 64.28133947686663\n",
        "* RMSE: 8.017564435467085\n",
        "* MAE: 5.453501121114831\n",
        "* R2: 0.5841408456219916\n",
        "* adjusted R2: 0.5734228261792595"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MqXDT76WmHq"
      },
      "source": [
        "The Mean Squared Error (MSE) is perhaps the simplest and most common loss function and used to check how close estimates or forecasts are to actual values. Lower the MSE, the closer is forecast to actual. This is used as a model evaluation measure for regression models and the lower value indicates a better fit.\n",
        "\n",
        "RMSE- Root mean square error or root mean square deviation is one of the most commonly used measures for evaluating the quality of predictions. It shows how far predictions fall from measured true values using Euclidean distance.\n",
        "\n",
        "MAE- Mean absolute error refers to the magnitude of difference between the prediction of an observation and the true value of that observation.\n",
        "\n",
        "R2- The coefficient of determination, or R2 , is a measure that provides information about the goodness of fit of a model. In the context of regression it is a statistical measure of how well the regression line approximates the actual data.\n",
        "\n",
        "Adjusted R2: It is always lower than R2 as it adjusts for the increasing predictors and only shows improvement if there is a real improvement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcRj8ExtxcGU"
      },
      "source": [
        "This all are evluation metrics for linear regression model. More the R2 score close to 1 the more accurate our model will be and we got 0.57 in our test dataset that is 57% varaince in Y predictable from X."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qY1EAkEfxKe"
      },
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NLT3YBXkG0y"
      },
      "source": [
        "There is no alpha value in Linear regression. We use hyperparameter tuning next 2 algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      },
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "negyGRa7fxKf"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfvqoZmBfxKf"
      },
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaLui8CcfxKf"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      },
      "source": [
        "### 2- Lasso Regression (L1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7r-p3V8bsXU"
      },
      "source": [
        "Lasso regression is a regularization technique. It is used over regression methods for a more accurate prediction. This model uses shrinkage.It uses L1 regularization technique."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWYfwnehpsJ1"
      },
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "outputs": [],
      "source": [
        "# importing lasso regressor\n",
        "from sklearn.linear_model import Lasso\n",
        "lasso = Lasso(alpha=1.0, max_iter=3000)\n",
        "\n",
        "# Fit the Lasso model\n",
        "lasso.fit(X_train, y_train)\n",
        "\n",
        "# Create the model score\n",
        "print(lasso.score(X_test, y_test), lasso.score(X_train, y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwi62Sv9k4xj"
      },
      "outputs": [],
      "source": [
        "# Predict on the model\n",
        "# Making predictions on train and test data\n",
        "y_pred_train_lasso=lasso.predict(X_train)\n",
        "y_pred_test_lasso=lasso.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETz35tSok76b"
      },
      "outputs": [],
      "source": [
        "# the metrics for evaluations for train dataset:\n",
        "lasso_train_mse=mean_squared_error(y_train,y_pred_train_lasso)\n",
        "print('MSE:',lasso_train_mse)\n",
        "lasso_train_rmse=np.sqrt(lasso_train_mse)\n",
        "print('RMSE:',lasso_train_rmse)\n",
        "\n",
        "lasso_train_mae=mean_absolute_error(y_train,y_pred_train_lasso)\n",
        "print('MAE:',lasso_train_mae)\n",
        "\n",
        "#defining the R2 and adjusted R2 score for tain dataset.\n",
        "lasso_train_r2=r2_score(y_train,y_pred_train_lasso)\n",
        "print('R2:',lasso_train_r2)\n",
        "\n",
        "Adjusted_R2_l = (1-(1-r2_score(y_train, y_pred_train_lasso))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_lasso))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x51hlLLolCBy"
      },
      "outputs": [],
      "source": [
        "# the metrics for evaluations for test dataset:\n",
        "lasso_test_mse=mean_squared_error(y_test,y_pred_test_lasso)\n",
        "print('MSE:',lasso_test_mse)\n",
        "lasso_test_rmse=np.sqrt(lasso_test_mse)\n",
        "print('RMSE:',lasso_test_rmse)\n",
        "\n",
        "lasso_test_mae=mean_absolute_error(y_test,y_pred_test_lasso)\n",
        "print('MAE:',lasso_test_mae)\n",
        "\n",
        "#defining the R2 and adjusted R2 score for test dataset.\n",
        "lasso_test_r2=r2_score(y_test,y_pred_test_lasso)\n",
        "print('R2:',lasso_test_r2)\n",
        "\n",
        "Adjusted_R2_l = (1-(1-r2_score(y_test, y_pred_test_lasso))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_test, y_pred_test_lasso))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-cIKPQUlGsd"
      },
      "outputs": [],
      "source": [
        "#checking for heteroscedasticity\n",
        "residuals_train_lasso = y_train - y_pred_train_lasso\n",
        "residuals_test_lasso = y_test - y_pred_test_lasso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ff16xDNemSac"
      },
      "outputs": [],
      "source": [
        "round((np.mean(residuals_train_lasso)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxKvrMG2lONP"
      },
      "outputs": [],
      "source": [
        "#Comparing predictions with actual values\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.plot(y_pred_test[0:100])\n",
        "plt.plot(np.array(y_test[0:100]))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRSjxhqeyOI7"
      },
      "source": [
        "I implement Lasso regression also called as L1 regularization as 2nd  machine learning algorithm. Again we use same evaluation metric as we did in our first ML model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUYjzf7zfRu0"
      },
      "source": [
        "For training dataset, I found out the following:\n",
        "* MSE: 89.51975299485277\n",
        "* RMSE: 9.461487884833588\n",
        "* MAE: 7.204583496116352\n",
        "* R2: 0.40670752151843703\n",
        "* Adjusted R2 : 0.39141647825860304\n",
        "\n",
        "For testing dataset, I found out the following:\n",
        "* MSE: 94.51550430578659\n",
        "* RMSE: 9.721908470346067\n",
        "* MAE: 7.399134746110156\n",
        "* R2: 0.3885451358654338\n",
        "* Adjusted R2 : 0.37278598988258427"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENebCuaufvMz"
      },
      "source": [
        "This all are evluation metrics for lasso regression model. \n",
        "\n",
        "we got R2 score around 0.14 which is not good result. The more R2 score is close to 1 , the more our model predicted to be accurate.\n",
        "\n",
        "Let's do some cross validation and hypertuning for better result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      },
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "outputs": [],
      "source": [
        "# ML Model - 2 Implementation with hyperparameter optimization techniques ( GridSearch CV)\n",
        "lasso = Lasso()\n",
        "parameters = {'alpha': [1e-15,1e-13,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1e-1,1,5,10,20,30,40,45,50,55,60,100]}\n",
        "lasso_regressor = GridSearchCV(lasso, parameters, scoring='neg_mean_squared_error', cv=3)\n",
        "lasso_regressor.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ixg64VyDgdnq"
      },
      "source": [
        "Using grid search cv for finding out the best parameter i.e alpha value for hyper meter tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-t00kxgIlvR8"
      },
      "outputs": [],
      "source": [
        "print(\"The best fit alpha value is found out to be :\" ,lasso_regressor.best_params_)\n",
        "print(\"\\nUsing \",lasso_regressor.best_params_, \" the negative mean squared error is: \", lasso_regressor.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bp7aCusl2dX"
      },
      "outputs": [],
      "source": [
        "#making prediction\n",
        "y_pred_lasso = lasso_regressor.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_DqroRtl5LA"
      },
      "outputs": [],
      "source": [
        "#Comparing predictions with actual values\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.plot(y_pred_test[0:100])\n",
        "plt.plot(np.array(y_test[0:100]))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLy8Bly3o5hR"
      },
      "outputs": [],
      "source": [
        "MSE  = mean_squared_error((y_test), (y_pred_lasso))\n",
        "print(\"MSE :\" , MSE)\n",
        "\n",
        "RMSE = np.sqrt(MSE)\n",
        "print(\"RMSE :\" ,RMSE)\n",
        "\n",
        "r2 = r2_score((y_test), (y_pred_lasso))\n",
        "print(\"R2 :\" ,r2)\n",
        "print(\"Adjusted R2 : \",1-(1-r2_score((y_test), (y_pred_lasso)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAih1iBOpsJ2"
      },
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      },
      "source": [
        "GridSearchCV which uses the Grid Search technique for finding the optimal hyperparameters to increase the model performance.\n",
        "Instead of doing hit and trial method for finding the best hypermeter we use gridsearchcv.\n",
        "\n",
        "Grid Search uses a different combination of all the specified hyperparameters and their values and calculates the performance for each combination and selects the best value for the hyperparameters. This makes the processing time-consuming and expensive based on the number of hyperparameters involved.\n",
        "\n",
        "In GridSearchCV, along with Grid Search, cross-validation is also performed. Cross-Validation is used while training the model.\n",
        "\n",
        "That's why I have used GridsearCV method for hyperparameter optimization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      },
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74yRdG6UpsJ3"
      },
      "source": [
        "Yeah, there is clear improvement in R2 score. After tuning the parameters by using gridsearchcv our R2 score improves a lot and it comes around 0.57 from 0.37. \n",
        "Now our R2 score is more closer to 1 that concludes more accuracy in our machine learning model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fze-IPXLpx6K"
      },
      "source": [
        "### ML Model - 3 - Ridge Regression (L2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HozZUlqFmViE"
      },
      "source": [
        "Ridge regression is a model tuning method that is used to analyse any data that suffers from multicollinearity. This method performs L2 regularization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "outputs": [],
      "source": [
        "# importing ridge regressor\n",
        "from sklearn.linear_model import Ridge\n",
        "ridge=Ridge(alpha=0.1)\n",
        "\n",
        "# Fit the Algorithm\n",
        "ridge.fit(X_train,y_train)\n",
        "ridge.score(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2yBjLM-qet5"
      },
      "outputs": [],
      "source": [
        "# Making predictions on train and test data\n",
        "y_pred_train_ridge=ridge.predict(X_train)\n",
        "y_pred_test_ridge=ridge.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wVugfnWqgcV"
      },
      "outputs": [],
      "source": [
        "# the metrics for evaluations for train dataset:\n",
        "ridge_train_mse=mean_squared_error(y_train,y_pred_train_ridge)\n",
        "print('MSE:',ridge_train_mse)\n",
        "ridge_train_rmse=np.sqrt(ridge_train_mse)\n",
        "print('RMSE:',ridge_train_rmse)\n",
        "\n",
        "ridge_train_mae=mean_absolute_error(y_train,y_pred_train_ridge)\n",
        "print('MAE:',ridge_train_mae)\n",
        "\n",
        "#defining the R2 score for tain dataset.\n",
        "ridge_train_r2=r2_score(y_train,y_pred_train_ridge)\n",
        "print('R2:',ridge_train_r2)\n",
        "\n",
        "#making adjusted R2 score for train dataset.\n",
        "Adjusted_R2_l = (1-(1-r2_score(y_train, y_pred_train_ridge))*((X_train.shape[0]-1)/(X_train.shape[0]-X_train.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_ridge))*((X_train.shape[0]-1)/(X_train.shape[0]-X_train.shape[1]-1)) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DP1hfjOqjZB"
      },
      "outputs": [],
      "source": [
        "#defining the metrics for evaluations for test dataset:\n",
        "ridge_test_mse=mean_squared_error(y_test,y_pred_test_ridge)\n",
        "print('MSE:',ridge_test_mse)\n",
        "ridge_test_rmse=np.sqrt(ridge_test_mse)\n",
        "print('RMSE:',ridge_test_rmse)\n",
        "\n",
        "ridge_test_mae=mean_absolute_error(y_test,y_pred_test_ridge)\n",
        "print('MAE:',ridge_test_mae)\n",
        "\n",
        "#R2 and adjusted R2 score for test dataset:\n",
        "ridge_test_r2=r2_score(y_test,y_pred_test_ridge)\n",
        "print('R2:',ridge_test_r2)\n",
        "\n",
        "Adjusted_R2_l = (1-(1-r2_score(y_test, y_pred_test_ridge))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_test, y_pred_test_ridge))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4sBuYzWrBTt"
      },
      "outputs": [],
      "source": [
        "#checking for heteroscedasticity\n",
        "residuals_train_ridge = y_train - y_pred_train_ridge\n",
        "residuals_test_ridge = y_test - y_pred_test_ridge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDZ-f_bkrMck"
      },
      "outputs": [],
      "source": [
        "plt.subplots(figsize=(10,5))\n",
        "plt.scatter(y_pred_train_ridge, residuals_train_ridge,c='yellow')\n",
        "plt.plot(y, [0]*len(y),color='green')\n",
        "plt.title('Scatter plot between residual and actual count')\n",
        "plt.xlabel('Predicted count')\n",
        "plt.ylabel('Residual')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8H7ZwoSrPrd"
      },
      "outputs": [],
      "source": [
        "round((np.mean(residuals_train_ridge)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AN1z2sKpx6M"
      },
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "outputs": [],
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.plot((y_pred_test_ridge[0:100]))\n",
        "plt.plot((np.array(y_test[0:100])))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TewIobplmrlQ"
      },
      "source": [
        "As 3rd machine learning model, I used L2 regularization technique that is Ridge regression. \n",
        "\n",
        "For training dataset i found out following:\n",
        "* MSE: 56.06032593929304\n",
        "* RMSE: 7.4873443849801005\n",
        "* MAE: 5.255146935845489\n",
        "* R2: 0.6284599922553425\n",
        "* Adjusted R2 : 0.6253229488985792\n",
        "\n",
        "For testing dataset i found out the following:\n",
        "* MSE: 64.27907333571898\n",
        "* RMSE: 8.017423110683318\n",
        "* MAE: 5.453434824955445\n",
        "* R2: 0.5841555061058745\n",
        "* Adjusted R2 : 0.5734378645106651"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sjdrp7QOnqjr"
      },
      "source": [
        "As from above evaluation metrics you can see that we got R2 score of 0.77 for training dataset and 0.78 for test dataset without any hyperparameter tunning which is already kind of better result for our model.\n",
        "The R2 score is more closer to 1 , the more your model predictions are going to be accurate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3HiFm6_oTkJ"
      },
      "source": [
        "Let's apply cross validation and hyperparameter tunning in our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfnJmrFi0Ac6"
      },
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "outputs": [],
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "#let's apply gridserachcv method for finding best fit parameter or alpha value.\n",
        "ridge = Ridge()\n",
        "parameters = {'alpha': [1e-15,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1,5,10,20,30,40,45,50,55,60,100]}\n",
        "ridge_regressor = GridSearchCV(ridge, parameters, scoring='neg_mean_squared_error', cv=3)\n",
        "ridge_regressor.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vopRjXaRtDMM"
      },
      "outputs": [],
      "source": [
        "#printing the best aplha value given by gridserachcv function.\n",
        "print(\"The best fit alpha value is found out to be :\" ,ridge_regressor.best_params_)\n",
        "print(\"\\nUsing \",ridge_regressor.best_params_, \" the negative mean squared error is: \", ridge_regressor.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ff4TKr_AtLbe"
      },
      "outputs": [],
      "source": [
        "# Predict on the model\n",
        "y_pred_ridge = ridge_regressor.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUPv0TCNtMg4"
      },
      "outputs": [],
      "source": [
        "#metrics for model evaluation in ridge regression:\n",
        "MSE  = mean_squared_error((y_test), (y_pred_ridge))\n",
        "print(\"MSE :\" , MSE)\n",
        "\n",
        "RMSE = np.sqrt(MSE)\n",
        "print(\"RMSE :\" ,RMSE)\n",
        "\n",
        "r2 = r2_score((y_test), (y_pred_ridge))\n",
        "print(\"R2 :\" ,r2)\n",
        "print(\"Adjusted R2 : \",1-(1-r2_score((y_test), (y_pred_ridge)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RWnsUkcty9Q"
      },
      "outputs": [],
      "source": [
        "#Comparing predictions with actual values\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot((y_pred_ridge[0:100]))\n",
        "plt.plot((np.array(y_test[0:100])))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-qAgymDpx6N"
      },
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQMffxkwpx6N"
      },
      "source": [
        "GridSearchCV which uses the Grid Search technique for finding the optimal hyperparameters to increase the model performance. Instead of doing hit and trial method for finding the best hypermeter we use gridsearchcv.\n",
        "\n",
        "Grid Search uses a different combination of all the specified hyperparameters and their values and calculates the performance for each combination and selects the best value for the hyperparameters. This makes the processing time-consuming and expensive based on the number of hyperparameters involved.\n",
        "\n",
        "In GridSearchCV, along with Grid Search, cross-validation is also performed. Cross-Validation is used while training the model.\n",
        "\n",
        "That's why I have used GridsearCV method for hyperparameter optimization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-hykwinpx6N"
      },
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzVzZC6opx6N"
      },
      "source": [
        "There is no slight changes after applying the hyperparameter tuning and cross validation.\n",
        "* R2 score before cross valdiation and hyperparameter tuning: 0.57\n",
        "* R2 score after appying cross valdiation and hyperparameter tuning:0.57\n",
        "which is same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMclNmRJA_KO"
      },
      "source": [
        "### Random Forest Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqwC8v4u2ugA"
      },
      "source": [
        "Random forest is a Supervised Machine Learning Algorithm that is used widely in Classification and Regression problems. It builds decision trees on different samples and takes their majority vote for classification and average in case of regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72GKZWewBfDJ"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the RandomForestClassifier\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "ensemble_regressor = RandomForestRegressor()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVNDkqA3H5-U"
      },
      "outputs": [],
      "source": [
        " #Fit the Algorithm\n",
        "ensemble_regressor.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wos4uqOtH68J"
      },
      "outputs": [],
      "source": [
        "# Predict on the model\n",
        "# Making predictions on train and test datay_pred_train_rf = ensemble_regressor.predict(X_train)\n",
        "y_pred_test_rf = ensemble_regressor.predict(X_test)\n",
        "y_pred_train_rf = ensemble_regressor.predict(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAF4bmK-Ip__"
      },
      "outputs": [],
      "source": [
        "# Calculating accuracy on train set.\n",
        "rf_train_mse=mean_squared_error(y_train,y_pred_train_rf)\n",
        "print('MSE:',rf_train_mse)\n",
        "rf_train_rmse=np.sqrt(rf_train_mse)\n",
        "print('RMSE:',rf_train_rmse)\n",
        "\n",
        "rf_train_mae=mean_absolute_error(y_train,y_pred_train_rf)\n",
        "print('MAE:',rf_train_mae)\n",
        "\n",
        "rf_train_r2=r2_score(y_train,y_pred_train_rf)\n",
        "print('R2:',rf_train_r2)\n",
        "\n",
        "Adjusted_R2_l = (1-(1-r2_score(y_train, y_pred_train_rf))*((X_train.shape[0]-1)/(X_train.shape[0]-X_train.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_rf))*((X_train.shape[0]-1)/(X_train.shape[0]-X_train.shape[1]-1)) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyAkYzWuNAbM"
      },
      "outputs": [],
      "source": [
        "# Calculating accuracy on train set.\n",
        "rf_test_mse=mean_squared_error(y_test,y_pred_test_rf)\n",
        "print('MSE:',rf_test_mse)\n",
        "rf_test_rmse=np.sqrt(rf_test_mse)\n",
        "print('RMSE:',rf_test_rmse)\n",
        "\n",
        "rf_test_mae=mean_absolute_error(y_test,y_pred_test_rf)\n",
        "print('MAE:',rf_test_mae)\n",
        "\n",
        "rf_test_r2=r2_score(y_test,y_pred_test_rf)\n",
        "print('R2:',rf_test_r2)\n",
        "\n",
        "Adjusted_R2_l = (1-(1-r2_score(y_test, y_pred_test_rf))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_test, y_pred_test_rf))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7-mZhO0Otpt"
      },
      "outputs": [],
      "source": [
        "#checking for heteroscedasticity\n",
        "residuals_train_rf = y_train - y_pred_train_rf\n",
        "residuals_test_rf = y_test - y_pred_test_rf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNNngsTHO6LT"
      },
      "outputs": [],
      "source": [
        "plt.subplots(figsize=(10,5))\n",
        "plt.scatter(y_pred_train_rf, residuals_train_rf,c='yellow')\n",
        "plt.plot(y, [0]*len(y),color='green')\n",
        "plt.title('Scatter plot between residual and actual count')\n",
        "plt.xlabel('Predicted count')\n",
        "plt.ylabel('Residual')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i457fOYmPFH1"
      },
      "outputs": [],
      "source": [
        "round((np.mean(residuals_train_rf)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LK-TvGHVRYuw"
      },
      "outputs": [],
      "source": [
        "#Comparing predictions with actual values\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.plot((y_pred_test_rf[0:100]))\n",
        "plt.plot((np.array(y_test[0:100])))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eEPAR3s36Bx"
      },
      "source": [
        "For training dataset i found out following:\n",
        "* MSE: 4.739914154372811\n",
        "* RMSE: 2.1771343905172253\n",
        "* MAE: 1.2209318315423554\n",
        "* R2: 0.9685862022362888\n",
        "* Adjusted R2 : 0.9683209644596532\n",
        "\n",
        "For test dataset i found out following:\n",
        "* MSE: 42.2607041560889\n",
        "* RMSE: 6.500823344476367\n",
        "* MAE: 3.496719679965829\n",
        "* R2: 0.7266002725395141\n",
        "* Adjusted R2 : 0.7195538878111511"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_OFQaHH_GDS"
      },
      "source": [
        "From above metrics we can clearly see that how well random frorest classifier algorithm performed on our dataset. It capture 70% variance as our R2 score is near 0.7.\n",
        "\n",
        "Let's apply cross validation and hyperparameter tuning techniques on model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ol3T38xC0OCM"
      },
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CJoMZNkToGF"
      },
      "outputs": [],
      "source": [
        "#ML Model - 2 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Grid search\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "rf_cv = RandomForestRegressor(random_state=42)\n",
        "param_dict = {'n_estimators':[50,80,100],\n",
        "              'max_depth':[4,6,8],\n",
        "              'min_samples_split':[50,100,150],\n",
        "              'min_samples_leaf':[40,50]}\n",
        "rf_reg = GridSearchCV(rf_cv, param_dict, verbose=1, scoring='neg_mean_squared_error', cv=5)\n",
        "rf_reg.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiu1iq_JUUgO"
      },
      "outputs": [],
      "source": [
        "rf_reg_best_est=rf_reg.best_estimator_\n",
        "print(f'The best estimator values : {rf_reg_best_est}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ocy6XrMVbjP"
      },
      "outputs": [],
      "source": [
        "# best fit values\n",
        "rf_reg_best_params=rf_reg.best_params_\n",
        "print(f'The best fit values: {rf_reg_best_params}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Yz2rP0NVfAh"
      },
      "outputs": [],
      "source": [
        "rf_reg_score=rf_reg.best_score_\n",
        "print(f\" The negative mean squared error is: {rf_reg_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pi9Dn4uViS4"
      },
      "outputs": [],
      "source": [
        "# Predicting results for train set\n",
        "y_train_pred=rf_reg.predict(X_train)\n",
        "y_test_pred=rf_reg.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3B38Z1-FVuHQ"
      },
      "outputs": [],
      "source": [
        "rf_train_mse=mean_squared_error(y_train,y_train_pred)\n",
        "print('MSE:',rf_train_mse)\n",
        "rf_train_rmse=np.sqrt(rf_train_mse)\n",
        "print('RMSE:',rf_train_rmse)\n",
        "\n",
        "rf_train_mae=mean_absolute_error(y_train,y_train_pred)\n",
        "print('MAE:',rf_train_mae)\n",
        "\n",
        "rf_train_r2=r2_score(y_train,y_train_pred)\n",
        "print('R2:',rf_train_r2)\n",
        "\n",
        "Adjusted_R2_l = (1-(1-r2_score(y_train, y_train_pred))*((X_train.shape[0]-1)/(X_train.shape[0]-X_train.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_train_pred))*((X_train.shape[0]-1)/(X_train.shape[0]-X_train.shape[1]-1)) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jpas2QSwYIRa"
      },
      "outputs": [],
      "source": [
        "# Predicting results for test set\n",
        "rf_test_mse=mean_squared_error(y_test,y_test_pred)\n",
        "print('MSE:',rf_test_mse)\n",
        "rf_test_rmse=np.sqrt(rf_test_mse)\n",
        "print('RMSE:',rf_test_rmse)\n",
        "\n",
        "rf_test_mae=mean_absolute_error(y_test,y_test_pred)\n",
        "print('MAE:',rf_test_mae)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5r4oK-1Uatks"
      },
      "outputs": [],
      "source": [
        "rf_testr2=r2_score(y_test,y_test_pred)\n",
        "print('R2:',rf_testr2)\n",
        "\n",
        "Adjusted_R2_l = (1-(1-r2_score(y_test, y_test_pred))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_test, y_test_pred))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0N1Jo13zxfJ"
      },
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMC-zHAv3rxA"
      },
      "source": [
        "GridSearchCV which uses the Grid Search technique for finding the optimal hyperparameters to increase the model performance. Instead of doing hit and trial method for finding the best hypermeter we use gridsearchcv.\n",
        "\n",
        "Grid Search uses a different combination of all the specified hyperparameters and their values and calculates the performance for each combination and selects the best value for the hyperparameters. This makes the processing time-consuming and expensive based on the number of hyperparameters involved.\n",
        "\n",
        "In GridSearchCV, along with Grid Search, cross-validation is also performed. Cross-Validation is used while training the model.\n",
        "\n",
        "That's why I have used GridsearCV method for hyperparameter optimization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1NVqDa2z5Qs"
      },
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx0u52OV3zBX"
      },
      "source": [
        "We got following for training dataset:\n",
        "* MSE: 52.29924946937519\n",
        "* RMSE: 7.231822002052815\n",
        "* MAE: 4.959074771210912\n",
        "* R2: 0.6533865398154609\n",
        "* Adjusted R2 : 0.650459960093301\n",
        "\n",
        "For test dataset i found out following:\n",
        "* MSE: 63.84935045360341\n",
        "* RMSE: 7.990578855978045\n",
        "* MAE: 5.398503832161154\n",
        "* R2: 0.5869355383178319\n",
        "* Adjusted R2 :  0.5762895470373637\n",
        "\n",
        "\n",
        "After applying the gridserchcv which is both cross validation and hyperparameter tuning technique, we get sightly less R2 score as it make our model prediction more realistic and avoid some kind of overfiiting.\n",
        "Previously our model was succesfully capturing the 70% of variance but after applying the gridsearch cv our model capturing 57% of variance which looks more accurate and realistic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_CCil-SKHpo"
      },
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHVz9hHDKFms"
      },
      "source": [
        "As it is simple regression problem, I would like to go with Adjusted R2 score which describe the how much the proportion of the variance in the dependent variable that is predictable from the independent variable(s). R2 score is a metric that tells the performance of your model.\n",
        "\n",
        "**What is a good R2 score in machine learning?**\n",
        "\n",
        "If the value of the r squared score is 1, it means that the model is perfect and if its value is 0, it means that the model will perform badly on an unseen dataset. This also implies that the closer the value of the r squared score is to 1, the more perfectly the model is trained."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBFFvTBNJzUa"
      },
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fKs_wdbZKv6"
      },
      "outputs": [],
      "source": [
        "# import module\n",
        "from tabulate import tabulate\n",
        "\n",
        "\n",
        "# Assign data\n",
        "mydata = [\n",
        "    ['Linear regression',\"64.28133947686663\", \"8.017564435467085\",'0.5841408456219916','0.5734228261792595'],\n",
        "    ['Lasso (L1)',\"64.25330858848494\", \"8.015816152363085\", ' 0.5843221875422712','0.573608841860371'],\n",
        "    ['Ridge (L2)',\"64.1733537176787\", \"8.010827280479756\", '0.5848394444201224','0.5741394301010534'],\n",
        "    ['Random Forest Classifier',\"63.84935045360341\", \"7.990578855978045\", '0.5869355383178319','0.5762895470373637']\n",
        "]\n",
        " \n",
        "# Create header for table\n",
        "head = ['ML Model Name',\"MSE\", \"RMSE\",'R-squared','Ad. R-squared']\n",
        " \n",
        "# display table\n",
        "print(tabulate(mydata, headers=head, tablefmt=\"grid\"))\n",
        "print('Note : Highest evaluation metric are taken into consideration between train_test_split and cross validation from each model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvR_cuH1et-k"
      },
      "source": [
        "From above chart we get similar value in all of our models. \n",
        "But I have choosen Random Forest Clasifier model because it has more parameters in comparison to linear models and even though they have liitle lesser R2 value than Linear models but overall they have better avg accuracy than Linear models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvGl1hHyA_VK"
      },
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features = [i for i in new_df.columns if i not in ['Rented_bike_count']]\n",
        "def feature_importance(model):\n",
        "  try:\n",
        "    importance = model.feature_importances_\n",
        "    feature = features\n",
        "  except:\n",
        "    importance = np.abs(model.coef_)\n",
        "    feature = X\n",
        "  indices = np.argsort(importance)\n",
        "  indices = indices[::-1]\n",
        "  #plotting\n",
        "  plt.figure(figsize=(12,4))\n",
        "  plt.bar(range(len(indices)),importance[indices])\n",
        "  plt.xticks(range(len(indices)), [feature[i] for i in indices])\n",
        "  plt.xticks(rotation = 90)\n",
        "  plt.title('Feature Importance')\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "eYMM20Vk5PoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_importance(ensemble_regressor)"
      ],
      "metadata": {
        "id": "-WvhMyjC5R67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      },
      "source": [
        "We can use the Random Forest algorithm for feature importance implemented in scikit-learn as the RandomForestRegressor and RandomForestClassifier classes.\n",
        "\n",
        "After being fit, the model provides a **feature_importances_ property** that can be accessed to retrieve the relative importance scores for each input feature.\n",
        "\n",
        "This approach can also be used with the bagging and extra trees algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyNgTHvd2WFk"
      },
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KH5McJBi2d8v"
      },
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "outputs": [],
      "source": [
        "# Save the File"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      },
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "outputs": [],
      "source": [
        "# Load the File and predict unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Kee-DAl2viO"
      },
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCX9965dhzqZ"
      },
      "source": [
        "# **Conclusion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      },
      "source": [
        "Write the conclusion here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIfDvo9L0UH2"
      },
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "mDgbUHAGgjLW",
        "HhfV-JJviCcP",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "626xLvZ0ZOga",
        "KSlN3yHqYklG",
        "4Of9eVA-YrdM",
        "x-EpHcCOp1ci",
        "n3dbpmDWp1ck",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "49K5P_iCpZyH",
        "kLW572S8pZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "89xtkJwZ18nB",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "1UUpS68QDMuG",
        "T5CmagL3EC8N",
        "P1XJ9OREExlT",
        "OB4l2ZhMeS1U",
        "4qY1EAkEfxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "nfnJmrFi0Ac6",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}